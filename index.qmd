---
title: "Cross-Validation1"
author: "Brenden Ackerson, Lindsey Cook, Dominic Gallelli"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features

---

## Introduction

Cross Validation is a key method used to assess model generalizability, which describes the extent which
statistical models developed in one sample fit other samples from the same population. [@song2021making].
Essentially the goal of cross validation is to mimic the prediction of future individuals from the
population. Pioneering work of Stone and Geisser in the 1970’s and the 1980’s by Burman on leave-one-
out cross validation set the stage for the current techniques of cross validation [@jung2015ak].
Common techniques of cross validation include data splitting (the most common method) by the hold-
out technique, random subsampling by Monte Carlo, k-folds and repeated k-folds, and leave-one-out
methods. [@ahmed2019classification].

The general process to cross validation is a 5 step process [@song2021making]:

1. Split the data into a training set and test set

2. Fit a model to the training set and obtain the model parameters

3. Apply the fitted model to the test set and obtain prediction accuracy

4. Repeat steps one through three

5. Calculate the average cross-validation prediction accuracy across all the repetitions

This process differs in the various cross-validation techniques by varying how the data is split and how
many repetitions are preformed of the train and test cycles. The best cross validation technique
considers the model’s bias or difference between the population parameter and the cross-validation
estimate, the variance or uncertainty in the cross-validation estimates, and with the computation costs
of the method.


## Methods

**Hold Out Cross- Validation Technique (Ahmed, 2019)**:This method tends to use defined splits for the
training and test set, like a 90/10, 80/20, 70/30, 60/40. Or 50/50 train/test data split. This method tends
to have issues with small data sets. [@jung2015ak]

**k-folds Cross-Validation Technique (Jung, 2015)**:

1. Randomly and evenly split the data set into k-folds.

2. Use k-1 folds of data as the training set to fit the model

3. With the fitted models, predict the value of the response variable in the hold out fold (kth fold)

4. From the response variable in the hold-out fold, calculate the prediction error

5. Repeat steps 2-4 for k times, so each k fold is used as a hold-out

6. Compare the prediction performance measures to select the “best” model

[@jung2015ak]

Repeated k-folds Cross-Validation Technique [@song2021making]: Extends the k-folds by conducing multiple
repetitions, each using a different k fold split.  

**Leave-one-out Cross-Validation Technique (Derryberry, 2014)**: This method is a special case of the k-
folds technique that systematically excludes each point in the data set and fits the model with all other
n-1 points. This technique splits the data into sets of n-1 and 1, n times. For each observation, the cross-
validation residual is the difference between the observation and the model predicted value.[@derryberry2014basic]



**Monte Carlo Cross-Validation Technique (Song, 2021)**: Similar to a k-fold method, but a predefined
proportion of the data is randomly selected to form the test set in each prepetition, and the remaining
proportion forms the training set. The train-test process is repeated a predetermined number of times. [@wainer2021nested]

**Multiple Predicting Cross-Validation (MPCV)(Jung 2018)** : This is a variation of k-folds but instead of each fold being the validation set, it is the training set. The trained model is then evaluated on the remaining data. The average of the k-fold is then used to measure how good the model is. [@jung2018multiple]

**Optimal Number of Folds**: One big decision that has to be made when using cross-validation methods other than leave one out, is the value of k the number of sections to divide the data set into. In the article "Performance of Machine Learning Algorithms with Different K Values in K-fold Cross\-Validation"[@nti2021performance] k-validation was used on 4 different machine learning algorithms 3, 5, 7, 10, 15 and 20. They found the optimal k folds changed depending on the model being tested.[@nti2021performance]

**Consistency Limitation of cross-validation**: One of the limitations of cross-validation methods is that they are not guaranteed to pick the true model of the data distribution, even as the number of samples approaches infinity. This is because when you train two different models on the same training set the one with more free parameters will model the training data better because it can over fit the data more easily. This can result in picking the wrong model due to over fitting. [@gronau2019limitations]
## Analysis and Results
### Data and Vizualisation

This paper will look at two data sets and review the various cross-validation techniques on each linear model to determine which cross-validation method is optimal.

The first data set is WordsWithFriends (Stats2Data R package), and the linear model of WinMargin (the margin a player wins Words With Friends) will be modeled against various variables that could impact winning, such as if a player starts the game or passes, how many S titles and blank tiles he played, and if the player got the J, Q, X, or Z tiles. This data set contains 444 observations.

| Variable   Name     |     Type         |     Characteristic             |
|---------------------|------------------|--------------------------------|
|     WinMargin       |     Response     |     Numeric                    |
|     Start           |     Predictor    |     Categorical,   2 levels    |
|     Ss              |     Predictor    |     Numeric                    |
|     BlanksNumber    |     Predictor    |     Numeric                    |
|     J               |     Predictor    |     Categorical,   2 levels    |
|     Q               |     Predictor    |     Categorical,   2 levels    |
|     X               |     Predictor    |     Categorical,   2 levels    |
|     Z               |     Predictor    |     Categorical,   2 levels    |

[Link to Documentation for Wordwithfreinds data](https://www.rdocumentation.org/packages/Stat2Data/versions/2.0.0/topics/WordsWithFriends)


The next data set this paper will review is the R data set, Backpack (Stats2Data package). This data set has 100 observations and will model the backpack weight to body weight ratio to various predictors such as if a person has back problems, what major, year in school, and credits taken, along with the sex of the wearer and if he/she is an undergraduate or graduate student.

| Variable   Name     |     Type         |     Characteristic                             |
|---------------------|------------------|------------------------------------------------|
|     Ratio           |     Response     |     Numeric                                    |
|     BackProblems    |     Predictor    |     Categorical,   2 levels                    |
|     Major           |     Predictor    |     Categorical,   41 levels                   |
|     Year            |     Predictor    |     Numeric   (could change to categorical)    |
|     Sex             |     Predictor    |     Categorical,   2 levels                    |
|     Status          |     Predictor    |     Categorical,   2 levels                    |
|     Units           |     Predictor    |     Numeric                                    |

[Link to Documentation for backpack data](https://www.rdocumentation.org/packages/Stat2Data/versions/2.0.0/topics/Backpack)


```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(Stat2Data)
```


### Statistical Modeling

### Conlusion

## References



