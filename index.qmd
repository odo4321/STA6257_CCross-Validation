---
title: "Cross-Validation"
author: "Brenden Ackerson, Lindsey Cook, Dominic Gallelli"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features

---

## Introduction

Cross Validation is a key method used to assess model generalizability, which describes the extent which
statistical models developed in one sample fit other samples from the same population. [@song2021making].
Essentially the goal of cross validation is to mimic the prediction of future individuals from the
population. The early pioneering work of Stone and Geisser in the 1970’s and the work by Burman in the 1980's on leave-one-
out cross validation, set the stage for the current techniques of cross validation [@jung2015ak].
Today's common techniques of cross validation include data splitting (the most common method) by the hold-
out or validation technique, random subsampling by Monte Carlo, k-folds and repeated k-folds, and leave-one-out
methods. [@ahmed2019classification].

The general process to cross validation is a 5 step process [@song2021making]:

1. Split the data into a training set and test set
2. Fit a model to the training set and obtain the model parameters
3. Apply the fitted model to the test set and obtain prediction accuracy
4. Repeat steps one through three
5. Calculate the average cross-validation prediction accuracy across all the repetitions

This process differs in the various cross-validation techniques by varying how the data is split and how
many repetitions are preformed of the train and test cycles. Details for the various techniques are outlined in the methods section.  The best cross validation technique
considers the model’s bias or difference between the population parameter and the cross-validation
estimate, the variance or uncertainty in the cross-validation estimates, and the computation costs
associated with each method.

Typical technique comparisons for linear models use the root mean squared error (RMSE), mean absolute error (MAE), and R-squared (R2). RMSE calculates how far predicted values are from observed values in the data set and is calculated by: 	
$RMSE= \sqrt{\frac{\sum(P_i – O_i)^2}{n}}$,
with $P_i$ being the predicted value for the ith observation, $O_i$ being the ith observed value, and n being the sample size. MAE describes the typical magnitude of the residuals and is calculated by:
$MAE=\frac{1}{n}\sum(|y_i-\hat{y}|$
where $y_i$ is the actual output value and $\hat{y}$  is the predicted output. $R^2$ describes how well the model predictors explains the response variable variation, or the fraction of the variance that is explained in the model. This equation is as follows: $R^2=\frac{n\sum(xy)-\sum(x)\sum(y)}{\sqrt{[n\sum(x^2)-(\sum(x)^2)][n\sum(y^2)-(\sum(y)^2)]}}$. This can also be written as:
$R^2=1-\frac{\sum(y_i-\hat{y})^2}{\sum(y_i-\overline{y})^2}$, where $\overline{y}$ is the mean value of y.

## Methods

2.1  The most common method is the **Hold Out or Validation Technique** (Ahmed, 2019): This method tends to use defined splits for the training and test set, like a 90/10, 80/20, 70/30, 60/40, or 50/50 train/test data split. This method is considered an easy, straightforward approach, but with the model only being built on a portion of the data, this model may not predict well as it is sensitive to what data is (or isn’t) chosen in the training set and this is especially problematic for small sample sizes. For the car data set the method for Hold Out validation is as follows:

1.	Split the data into an 80/20 training/test data set.
2.	Fit the model to the training set
3.	Apply the fitted model to the test set
4.	Compare the model predictions
 

2.2 The **k-folds Cross-Validation Technique** is one potential answer to the limitations in the simple hold-out technique. The data is divided into k group or splits, and then each k group becomes a test set while the other groups as a whole are the training set. The detailed process is as follows (Jung, 2015):

1.	Randomly and evenly split the data set into k-folds. 
2.	Use k-1 folds of data as the training set to fit the model
3.	With the fitted models, predict the value of the response variable in the hold out fold (kth fold)
4.	From the response variable in the hold-out fold, calculate the prediction error
5.	Repeat steps 2-4 for k times, so each k fold is used as a hold-out
6.	Compare the prediction performance measures to select the “best” model
##Figure out pic in our paper
{r}
library(knitr)
![Figure 1: k-folds Visual Depiction. F.S, 2019]
```
```

Repeated k-folds Cross-Validation Technique [@song2021making]: Extends the k-folds by conducing multiple
repetitions, each using a different k fold split.  

**Leave-one-out Cross-Validation Technique (Derryberry, 2014)**: This method is a special case of the k-
folds technique that systematically excludes each point in the data set and fits the model with all other
n-1 points. This technique splits the data into sets of n-1 and 1, n times. For each observation, the cross-
validation residual is the difference between the observation and the model predicted value.[@derryberry2014basic]



**Monte Carlo Cross-Validation Technique (Song, 2021)**: Similar to a k-fold method, but a predefined
proportion of the data is randomly selected to form the test set in each prepetition, and the remaining
proportion forms the training set. The train-test process is repeated a predetermined number of times. [@wainer2021nested]

**Multiple Predicting Cross-Validation (MPCV)(Jung 2018)** : This is a variation of k-folds but instead of each fold being the validation set, it is the training set. The trained model is then evaluated on the remaining data. The average of the k-fold is then used to measure how good the model is. [@jung2018multiple]

**Optimal Number of Folds**: One big decision that has to be made when using cross-validation methods other than leave one out, is the value of k the number of sections to divide the data set into. In the article "Performance of Machine Learning Algorithms with Different K Values in K-fold Cross\-Validation"[@nti2021performance] k-validation was used on 4 different machine learning algorithms 3, 5, 7, 10, 15 and 20. They found the optimal k folds changed depending on the model being tested.[@nti2021performance]

**Consistency Limitation of cross-validation**: One of the limitations of cross-validation methods is that they are not guaranteed to pick the true model of the data distribution, even as the number of samples approaches infinity. This is because when you train two different models on the same training set the one with more free parameters will model the training data better because it can over fit the data more easily. This can result in picking the wrong model due to over fitting. [@gronau2019limitations]

**On the use of cross-validation for time series predictor evaluation**: When it comes to selecting a model for a time series, there is a divide when we evaluate  a machine learning model versus the standard forecasting. However, there are downsides to both practices. Traditional forecasting leaves out part of the data for testing thus never utilizing all of the data set in training. Evaluating a machine learning model applies cross-validation but does not always meet the necessary assumptions. This study attempts to bridge the gap between the two sides evaluating six model selection methods. Finally, they found that cross-validation techniques led to more robust models and argues that the blocked form of cross-validation should be the standard practice when evaluating a time series. [@bergmeir2012]

## Analysis and Results
### Data and Vizualisation

Cross-validation techniques of hold-out validation, k-folds, repeated k-folds, and leave-one-out were performed on the Kaggle used Car data set. This data set allowed us to model the selling price of a car given the variables of kilometers driven, the fuel type used, the year the car was manufactured, if the seller was an individual or dealership, and what type of transmission was in the car.   



| Variable   Name     |     Type         |     Characteristic             |
|---------------------|------------------|--------------------------------|
|     Selling_Price   |     Response     |     Numeric                    |
|     Year            |     Predictor    |     Numeric                    |
|     Kms_Driven      |     Predictor    |     Numeric                    |
|     Fuel_Type       |     Predictor    |    Categorical, 3 levels       |
|     Seller_Type     |     Predictor    |    Categorical, 2 levels       |
|     Transmission    |     Predictor    |    Categorical, 2 levels       |


[Link to Documentation for CarData data](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho?select=car+data.csv)





```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(readr)
library(ggfortify)
library(caret)
library(caTools)


set.seed(1)

car_data = read.csv("car data.csv")

# model normal linear model. 
model1 <- lm(Selling_Price ~  Kms_Driven + Fuel_Type + Seller_Type + Transmission + Year, data = car_data)
autoplot(model1)
summary(model1)

# leave one out
sample <- sample.split(car_data$Year, SplitRatio = 0.7)
train  <- subset(car_data, sample == TRUE)
test   <- subset(car_data, sample == FALSE)

model_train = lm(Selling_Price ~ Kms_Driven + Fuel_Type + Seller_Type + Transmission + Year, data = train)
test_pred = predict(model_train, test)

data.frame(R_squared = R2(test_pred, test$Selling_Price),
           RMSE = RMSE(test_pred, test$Selling_Price),
           MAE = MAE(test_pred, test$Selling_Price))
#  R_squared     RMSE      MAE
#  0.675032 3.066001 2.059764

#k-folds Cross-Validation
ctrl2 <- trainControl(method = "cv", number = 10)
cv_model2 <- train(Selling_Price ~ Kms_Driven + Fuel_Type + Seller_Type + Transmission + Year,
                   method = "lm",
                   data = car_data,
                   trControl = ctrl2)
print(cv_model2)

# RMSE      Rsquared   MAE     
#v3.177328  0.6034333  2.084711

#Leave-one-out Cross-Validation
ctrlLOOCV <- trainControl(method = "LOOCV")
LOO_model3 <- train(Selling_Price ~ Kms_Driven + Fuel_Type + Seller_Type + Transmission + Year,
                  method = "lm",
                   data = car_data,
                   trControl = ctrlLOOCV )
print(LOO_model3)

# RMSE      Rsquared   MAE     
# 3.380532  0.5563956  2.089128


#Monte Carlo Cross-Validation
ctrlLGOCV <- trainControl(method = "LGOCV", number = 10)
LGOCV_model <- train(Selling_Price ~ Kms_Driven + Fuel_Type + Seller_Type + Transmission + Year,
                    method = "lm",
                    data = car_data,
                    trControl = ctrlLOOCV )
print(LGOCV_model)

# RMSE      Rsquared   MAE     
# 3.380532  0.5563956  2.089128



```


### Statistical Modeling
**Hold Out Validation:**
```{r}
#cardata=read_csv("C:\\Users\\lmc27\\Downloads\\cardata.csv")
#cardata$Fuel_Type=as.factor(cardata$Fuel_Type)
#cardata$Seller_Type=as.factor(cardata$Seller_Type)
#cardata$Transmission=as.factor(cardata$Transmission)
#cardata$Owner=as.factor(cardata$Owner)

#mod1=lm(Selling_Price~Kms_Driven+Fuel_Type+Seller_Type+Transmission+Year, data=cardata)
#summary(mod1)
#data1=cardata[, c("Selling_Price", "Year", "Kms_Driven", "Fuel_Type", "Seller_Type", "Transmission")]
#set.seed(0)
#training=data1$Selling_Price %>% createDataPartition(p=.8, list=FALSE)
#train=data1[training,]
#test=data1[-training,]
#hold_mod=lm(Selling_Price~., data=train)
#prediction=hold_mod %>% predict(test)
#data.frame(Rsquared=R2(prediction, test$Selling_Price), RMSE=RMSE(prediction, test$Selling_Price), MAE=MAE(prediction, test$Selling_Price))
```

We performed a normal linear regression using the carData set trying to predicted the sells price using information about the car. 

$Selling_Price = Kms_Driven + Fuel_Type + Seller_Type + Transmission + Year$

We performed the linear regression using 4 different types of cross validation. 
 Monte Carlo, k-folds with k set to 10, leave one out and a train test split. 


|          | Monte Carlo | k-folds k=10 | leave one out | Hold Out |
|----------|-------------|--------------|---------------|----------|
| RMSE     | 3.380532    | 3.177328     | 3.380532      | 3.066001 |
| Rsquared | 0.5563956   | 0.6034333    | 0.5563956     | 0.675032 |
| MAE      | 2.089128    | 2.084711     | 2.089128      | 2.059764 |

the model that was varavied using a training set and validation set performed the best,


### Conlusion

## References



